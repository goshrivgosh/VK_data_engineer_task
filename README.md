# VK_data_engineer_task

### Выполненное тестовое задание разделено на уровни:
 - Минимальный (в папке minimum_functionallity)

Содержимое: script.py - скрипт, который сохраняет агрегирующий файл логов
и script_test.py - файл с тестами для script.py
 - Рекомендуемый (в папке recommended_functionality)

В ходе выполнения задания был запущен docker-контейнер и развернут web-server Airflow.
Инициализация и запуск происходил следующим образом:
- Добавлен docker-compose.yaml файл, Dockerfile и requiremens.txt
- Созданы папки для логов, dag'ов; а также config и plugin (по умолчанию)
- Выполнено следующее: 

```docker compose up airflow-init```

```docker compose up```

Папка dags содержит в себе немного модифицированный ```script.py``` (явно прописаны пути к папкам, взаимодействие с которыми
доступно через  Apache Airflow, а также к актуальному времени, которое является именем итогового файла, добавлено 3 часа из-за
различий времени на сервере и локальной машине), а также ```create_dag.py```, в котором прописана конфигурация DAG.

Таким образом DAG настроен на ежедневное выполнение скрипта в 7:00. При запуске DAGA(он может работать
как по расписанию, так и с помощью trigger) выполняется скритп аггрегации; источником файлов является папка ```input```
в директории dags; результат хранится в папке ```output``` в директории dags - для примера
в репозитории есть 2 файла в input и output, на которых можно проверить программу.

Также в проекте присутсвует измененный файл ```generate.py```, который генирирует новые логи; изменения внесены для удобства 
тестирования написанных программ.